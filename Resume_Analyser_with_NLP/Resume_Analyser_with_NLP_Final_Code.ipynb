{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e842f14-b36b-448d-9a76-f0a9a0e231e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "import docx2txt\n",
    "import mammoth\n",
    "import textract\n",
    "import spacy\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import os\n",
    "import io\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737d409-60bf-4f28-baf0-71113fc4fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_email(text):\n",
    "    # compile helps us to define a pattern for matching it in the text\n",
    "    r = re.compile(r'[A-Za-z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
    "    return r.findall(str(text))\n",
    "    \n",
    "def extract_phone_numbers(text):\n",
    "    \n",
    "    text_without_extra_spaces = re.sub(r'\\s+', '', text)\n",
    "    #text_without_extra_spaces_brac=re.sub(r'[()\\[\\]{}]', '',text_without_extra_spaces)\n",
    "    print(text_without_extra_spaces)\n",
    "    phone_regex = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)')\n",
    "    phone_numbers = re.findall(phone_regex, text_without_extra_spaces)\n",
    "    #print(text_without_extra_spaces_brac)\n",
    "    new_phone_numbers=[]\n",
    "    for i in phone_numbers:\n",
    "        i=re.sub(r'[()\\[\\]{}]', '',i)\n",
    "        if len(i)>7 and i[0]!='2' and i[0]!='1' and i[0]!='0':\n",
    "            new_phone_numbers.append(i) \n",
    "    return new_phone_numbers\n",
    "\n",
    "def get_name(txt):\n",
    "    stops=stopwords.words('english')\n",
    "    punc=string.punctuation\n",
    "    punc=[char for char in punc ]\n",
    "    stops+=punc\n",
    "    \n",
    "    skills=pd.read_csv(skills_data_path)\n",
    "    skills_name=pd.DataFrame(skills['Text'])\n",
    "    skill_val=skills_name.values\n",
    "    skills_val = skill_val.reshape(-1).tolist()\n",
    "    \n",
    "    split_string =[]\n",
    "    split_string = txt.split()\n",
    "    #print(split_string)\n",
    "    name=\" \"\n",
    "    for j in range(len(split_string)):\n",
    "        \n",
    "        w=split_string[j]\n",
    "        #print(j,w,pos_tag([w]))\n",
    "        if w.lower() not in stops and len(w)!=1 and len(w)!=2 and w.lower() not in skills_val and w.isalpha() and \"@\" not in w and \"gmail.com\" not in w :\n",
    "            #print(w,pos_tag([w]))\n",
    "            name=w+\" \"+split_string[j+1]\n",
    "            break\n",
    "    return name\n",
    "    \n",
    "def clean_institute_name(name):\n",
    "    # Remove leading/trailing whitespace\n",
    "    name = name.strip()\n",
    "    # Replace '\\t' and '\\n' with a space\n",
    "    name = re.sub(r'[\\t\\n]', ' ', name)\n",
    "    # Remove extra spaces\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    return name\n",
    "def extract_educational_institutes(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract educational institutes\n",
    "    educational_institutes = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ == \"ORG\"  and (\"university\" in entity.text.lower() or \"college\" in entity.text.lower() or \"institute\" in entity.text.lower()):  # Check if the entity is an organization\n",
    "            clean_name = clean_institute_name(entity.text)\n",
    "            educational_institutes.append(clean_name)\n",
    "\n",
    "    \n",
    "    return educational_institutes\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file,strict=False)\n",
    "        num_pages = len(reader.pages)\n",
    "        for page_number in range(num_pages):\n",
    "            page = reader.pages[page_number]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    with open(docx_path, 'rb') as docx_file:\n",
    "        result = mammoth.extract_raw_text(docx_file)\n",
    "        text = result.value\n",
    "    \n",
    "    return text\n",
    "\n",
    "def get_simple_pos(tag):\n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else: \n",
    "        return wordnet.NOUN\n",
    "        \n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def clean_resume(txt):\n",
    "    stops=stopwords.words('english')\n",
    "    punc=string.punctuation\n",
    "    punc=[char for char in punc ]\n",
    "    stops+=punc\n",
    "    removed_space=[]\n",
    "    tok=[]\n",
    "    clean_resume=[]\n",
    "    \n",
    "    for resume in txt : \n",
    "        text = re.sub(r'\\s+', ' ', resume)\n",
    "        removed_space.append(text)\n",
    "    #print(len(removed_space))\n",
    "    #print(removed_space)\n",
    "\n",
    "    for resume in removed_space:\n",
    "        word_tok=[]\n",
    "        word_tok= word_tokenize(resume)\n",
    "        tok.append(word_tok)\n",
    "    #print(len(tok))\n",
    "    #print(tok)\n",
    "       \n",
    "    \n",
    "    for resume in tok:\n",
    "        output_words=[]\n",
    "        for w in resume:\n",
    "            if w.lower() not in stops and len(w)!=2 and len(w)!=1 :\n",
    "                pos=pos_tag([w])               # passing the as it is important because if we lower it we might lose some information\n",
    "                clean_words=lemmatizer.lemmatize(w,pos=get_simple_pos(pos[0][1]))\n",
    "                output_words.append(clean_words.lower())\n",
    "        clean_resume.append(output_words)\n",
    "    \n",
    "    final_cleaned_res=[]\n",
    "    for i in clean_resume:\n",
    "        one_res=\" \".join(i)\n",
    "        final_cleaned_res.append(one_res)\n",
    "    \n",
    "    return final_cleaned_res\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#return fileTXT,file_names\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cfb97d-7f58-46a7-820b-53ac2f141df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyse_Resume(Resume_path,skills_data_path,JD_path):\n",
    "    \n",
    "    skills=pd.read_csv(skills_data_path)\n",
    "    skills_name=pd.DataFrame(skills['Text'])\n",
    "    skill_val=skills_name.values\n",
    "    skills_val = skill_val.reshape(-1).tolist()\n",
    "    \n",
    "    fileTXT=[]\n",
    "    file_names=[]\n",
    "    for filename in os.listdir(Resume_path):\n",
    "        contact_info=[]\n",
    "        text=\"\"\n",
    "        if filename.endswith(\".pdf\") or filename.endswith(\".PDF\"):\n",
    "            try:\n",
    "                text=extract_text_from_pdf(Resume_path+\"/\"+filename)\n",
    "                fileTXT.append(text)\n",
    "                \n",
    "                name = get_name(text)\n",
    "                contact_info.append(name)\n",
    "                phn=extract_phone_numbers(text)\n",
    "                contact_info.append(phn)\n",
    "                email=get_email(text)\n",
    "                contact_info.append(email)\n",
    "                contact_info.append(Resume_path+\"/\"+filename)\n",
    "                file_names.append(contact_info)\n",
    "                edu=extract_educational_institutes(text)\n",
    "                contact_info.append(edu)\n",
    "            \n",
    "            except Exception:\n",
    "                print(\"Error Reading pdf file :\" + filename)\n",
    "        \n",
    "        elif filename.endswith(\".docx\"):\n",
    "            try:\n",
    "                text=extract_text_from_docx(Resume_path+\"/\"+filename)\n",
    "                fileTXT.append(text)\n",
    "                \n",
    "                name = get_name(text)\n",
    "                contact_info.append(name)\n",
    "                phn=extract_phone_numbers(text)\n",
    "                contact_info.append(phn)\n",
    "                email=get_email(text)\n",
    "                contact_info.append(email)\n",
    "                contact_info.append(Resume_path+\"/\"+filename)\n",
    "                file_names.append(contact_info)\n",
    "                edu=extract_educational_institutes(text)\n",
    "                contact_info.append(edu)\n",
    "                \n",
    "\n",
    "            except Exception:\n",
    "                print(\"Error reading docx file:\" + filename)\n",
    "\n",
    "        elif filename.endswith(\".doc\"):\n",
    "            try:\n",
    "                text=textract.process(Resume_path+\"/\"+filename).decode('utf-8')\n",
    "                fileTXT.append(text)\n",
    "\n",
    "                name = get_name(text)\n",
    "                contact_info.append(name)\n",
    "                phn=extract_phone_numbers(text)\n",
    "                contact_info.append(phn)\n",
    "                email=get_email(text)\n",
    "                contact_info.append(email)\n",
    "                contact_info.append(Resume_path+\"/\"+filename)\n",
    "                file_names.append(contact_info)\n",
    "                edu=extract_educational_institutes(text)\n",
    "                contact_info.append(edu)\n",
    "            \n",
    "            except Exception:\n",
    "                print('Error reading .doc file :' + filename)\n",
    "\n",
    "\n",
    "    final_res=clean_resume(fileTXT)\n",
    "    jd=extract_text_from_pdf(JD_path)\n",
    "    final_jd= clean_resume([jd])\n",
    "    jd_by_words=word_tokenize(final_jd[0])\n",
    "    skills_from_jd=[]\n",
    "    for i in jd_by_words:\n",
    "        for j in skills_val:\n",
    "            if i==j:\n",
    "                skills_from_jd.append(i)\n",
    "    two_word_list=[]\n",
    "    for i in range(len(jd_by_words)-1):\n",
    "        two_words=\"\"\n",
    "        two_words=jd_by_words[i]+\" \"+jd_by_words[i+1]\n",
    "        two_word_list.append(two_words)\n",
    "    \n",
    "    skills_from_jd2=[]\n",
    "    for i in two_word_list:\n",
    "        for j in skills_val:\n",
    "            if i==j:\n",
    "                skills_from_jd2.append(i)\n",
    "    final_features_from_jd= skills_from_jd+skills_from_jd2\n",
    "    new_final_features_from_jd=set(final_features_from_jd)\n",
    "    new_final_features_from_jd_list=list(new_final_features_from_jd)\n",
    "    soft_new_final_features_from_jd_list=[]\n",
    "    tech_new_final_features_from_jd_list=[]\n",
    "    for j in range(len(new_final_features_from_jd)):\n",
    "        f=new_final_features_from_jd_list[j]\n",
    "        if ((skills[skills['Text']==f]['Label']).values[0])==1:\n",
    "            tech_new_final_features_from_jd_list.append(new_final_features_from_jd_list[j])\n",
    "        else:\n",
    "            soft_new_final_features_from_jd_list.append(new_final_features_from_jd_list[j])\n",
    "    combined_skills=tech_new_final_features_from_jd_list+soft_new_final_features_from_jd_list\n",
    "\n",
    "\n",
    "    final_res.append(final_jd[0])\n",
    "\n",
    "    new_final_res_with_prime_words=[]\n",
    "    new_file_names=[]\n",
    "    # for java developer\n",
    "    if 'java' in final_jd[0]:\n",
    "        for i in range(len(final_res)-1):\n",
    "            splited_res=[]\n",
    "            splited_res=final_res[i].split()\n",
    "            if 'java' in splited_res:\n",
    "                new_final_res_with_prime_words.append(final_res[i])\n",
    "                new_file_names.append(file_names[i])\n",
    "        new_final_res_with_prime_words.append(final_jd[0])\n",
    "    elif 'legal' in final_jd[0]:\n",
    "        for i in range(len(final_res)-1):\n",
    "            splited_res=[]\n",
    "            splited_res=final_res[i].split()\n",
    "            if 'legal' in splited_res:\n",
    "                new_final_res_with_prime_words.append(final_res[i])\n",
    "                new_file_names.append(file_names[i])\n",
    "        new_final_res_with_prime_words.append(final_jd[0])\n",
    "    \n",
    "    elif 'graphic' in final_jd[0]:\n",
    "        for i in range(len(final_res)-1):\n",
    "            splited_res=[]\n",
    "            splited_res=final_res[i].split()\n",
    "            if 'graphic' in splited_res:\n",
    "                new_final_res_with_prime_words.append(final_res[i])\n",
    "                new_file_names.append(file_names[i])\n",
    "        new_final_res_with_prime_words.append(final_jd[0])\n",
    "    \n",
    "    else:\n",
    "        new_final_res_with_prime_words=final_res\n",
    "        new_file_names=file_names\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer(vocabulary = combined_skills,binary=True,ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(new_final_res_with_prime_words)\n",
    "    X_array = X.toarray()\n",
    "\n",
    "    # for making feature columns ( that will have skills that are present in the resume)\n",
    "    soft_skill_feature=[]\n",
    "    tech_skill_feature=[]\n",
    "    for i in range(len(X_array)-1):\n",
    "        soft_skill_feature_for_one_resume=[]\n",
    "        tech_skill_feature_for_one_resume=[]\n",
    "        for j in range(len(new_final_features_from_jd)):\n",
    "            f=new_final_features_from_jd_list[j]\n",
    "    \n",
    "            if X_array[i][j]==1:\n",
    "                if ((skills[skills['Text']==f]['Label']).values[0])==1:\n",
    "                    tech_skill_feature_for_one_resume.append(new_final_features_from_jd_list[j])\n",
    "                else:\n",
    "                    soft_skill_feature_for_one_resume.append(new_final_features_from_jd_list[j])\n",
    "                    \n",
    "        soft_skill_feature.append(soft_skill_feature_for_one_resume)\n",
    "        tech_skill_feature.append(tech_skill_feature_for_one_resume)\n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    similarity =cosine_similarity(X_array)\n",
    "    final_similarity=similarity[:,len(new_final_res_with_prime_words)-1]\n",
    "    for i in range(len(final_similarity)-1):\n",
    "        new_file_names[i].append(final_similarity[i])\n",
    "        new_file_names[i].append(soft_skill_feature[i])\n",
    "        new_file_names[i].append(tech_skill_feature[i])\n",
    "    final_df=pd.DataFrame(new_file_names,columns=[\"Name\",'Contact Number','Email Address','File Location','Education','Score','Soft Skills','Tech Skill'])\n",
    "    df_sorted = final_df.sort_values(by=['Score'],ascending=False)\n",
    "    df_sorted.to_csv('data1.csv', index=False)\n",
    "    return df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008dde77-4a0a-4161-b017-3c3ede13ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Resume_path='/Users/navneetyadav/Downloads/All_resume_new'\n",
    "skills_data_path='/Users/navneetyadav/Downloads/archive 2/all_data_skill_and_nonskills/all_data_skill_and_nonskills.csv'\n",
    "JD_path='/Users/navneetyadav/Downloads/JD_Java_Developer.pdf'\n",
    "\n",
    "df=Analyse_Resume(Resume_path,skills_data_path,JD_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d8611-a750-4ac9-967d-83b6acb1c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "names='/Users/navneetyadav/Downloads/All_resume_new/Legal_Resume- Anjali Sharma..pdf'\n",
    "if names.endswith(\".pdf\") or filename.endswith(\".PDF\"):\n",
    "    print('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c05702-d69d-4787-b1d1-ec808462eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9973bbcf-2e95-41cc-bdf0-d7c01b1d6842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
